{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import csv\n",
    "import pandas as pd\n",
    "from time import sleep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawlInfo(br, comp):\n",
    "    br.get(f'https://www.buffett-code.com/company/{comp}/')\n",
    "\n",
    "    # i+=1\n",
    "    # if i%60 == 0:\n",
    "    #     sleep(1500)\n",
    "    sleep(60)\n",
    "\n",
    "    if br.current_url == f'https://www.buffett-code.com/company/{comp}/':\n",
    "        \n",
    "        tables = []\n",
    "        try:\n",
    "            table_1 = br.find_element(By.XPATH, '/html/body/div[1]/div[2]/div/div[2]/div[4]/div/div[2]/table')\n",
    "            tables.append(table_1)\n",
    "        except:\n",
    "            print(comp, \"  table 1\")\n",
    "        \n",
    "        try:\n",
    "            table_2 = br.find_element(By.XPATH, '/html/body/div[1]/div[2]/div/div[2]/div[4]/div/div[5]/table')\n",
    "            tables.append(table_2)\n",
    "        except:\n",
    "            print(comp, \"  table 2\")\n",
    "        \n",
    "        try:\n",
    "            table_3 = br.find_element(By.XPATH, '/html/body/div[1]/div[2]/div/div[2]/div[10]/div/div[2]/table')\n",
    "            tables.append(table_3)\n",
    "        except:\n",
    "            print(comp, \"  table 3\")\n",
    "\n",
    "\n",
    "        dfs = [['Type'],['Value']]\n",
    "        for tb in tables:\n",
    "            \n",
    "            rows = tb.find_elements(By.XPATH, \".//tbody/tr\")\n",
    "            for row in range(1,len(rows)+1):\n",
    "                values = []\n",
    "                for col in range(1,3):\n",
    "                    dfs[col-1].append(tb.find_element(By.XPATH, f\".//tbody/tr[{row}]/td[{col}]\").text)\n",
    "        \n",
    "    else:\n",
    "        dfs = [['Type'],['Value']]\n",
    "\n",
    "    df_1 = pd.DataFrame(dfs[0])\n",
    "    df_2 = pd.DataFrame(dfs[1])\n",
    "\n",
    "    df_ = pd.concat([df_1,df_2], axis = 1)\n",
    "    df_.to_csv(f\"./CompanyInfo/ListingCompanyInfo/{comp}.csv\", header = False ,index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "br = webdriver.Edge()\n",
    "\n",
    "comps = pd.read_csv(\"./List_company_23052023 - Listing.csv\")\n",
    "lst_comp = comps['Symbol']\n",
    "\n",
    "i = 0\n",
    "# for comp in lst_comp[175:]:\n",
    "for comp in lst_comp[1234:]:\n",
    "    crawlInfo(br, comp)\n",
    "\n",
    "\n",
    "# dfs = []\n",
    "\n",
    "# for i in [1,4,26]:\n",
    "#     dfs.append(pd.read_html(br.page_source)[i])\n",
    "\n",
    "# df = pd.concat(dfs)\n",
    "# df.to_csv(f\"./CompanyInfo/{comp}.csv\", header = False ,index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for comp in [1333, 4586]:\n",
    "#     br.get(f'https://www.buffett-code.com/company/{comp}/')\n",
    "#     dfs = []\n",
    "\n",
    "#     for i in [1,4,26]:\n",
    "#         dfs.append(pd.read_html(br.page_source)[i])\n",
    "\n",
    "#     df = pd.concat(dfs)\n",
    "#     df.to_csv(f\"./CompanyInfo/{comp}.csv\", header = False ,index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
